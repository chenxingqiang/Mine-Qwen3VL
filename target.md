Qwen3-VL 8B Instruct é«˜å…‰è°±æ‰¾çŸ¿ä»»åŠ¡å¾®è°ƒ Pipelineï¼ˆå®Œæ•´è½åœ°ç‰ˆï¼‰

â¸»

0. Qwen3-VL çš„å…³é”®æŠ€æœ¯ç‰¹æ€§ï¼ˆåŸºäºæ–‡ä»¶ï¼‰

æ ¹æ®æŠ€æœ¯ç™½çš®ä¹¦ï¼š
	â€¢	Qwen3-VL ä½¿ç”¨ è§†è§‰ Transformer + token-level å¤šæ¨¡æ€å¯¹é½æœºåˆ¶
	â€¢	å›¾åƒè¾“å…¥ç»è¿‡ Vision Encoder â†’ Projector â†’ LLM æµå…¥
	â€¢	æ”¯æŒ å¯è®­ç»ƒ Projectorï¼ˆå³è§†è§‰ç‰¹å¾å’Œè¯­è¨€ç©ºé—´å¯¹é½å±‚ï¼‰
	â€¢	æ”¯æŒ åˆ†è¾¨ç‡è‡ªé€‚åº”ï¼Œæœ€é«˜è‡³ä¸Šç™¾ä¸‡åƒç´ è¾“å…¥
	â€¢	å…·å¤‡ å¼ºè§†è§‰è¡¨è¾¾èƒ½åŠ›ï¼Œå¯é€šè¿‡å¾®è°ƒå­¦ä¹ æ–°æ¨¡æ€

è¿™æ„å‘³ç€ï¼š
ğŸ‘‰ ä½ å®Œå…¨å¯ä»¥è®© Qwen3-VL å­¦ä¼šâ€œé«˜å…‰è°±å½±åƒï¼ˆä¼ªè‰²/ç‰¹å¾å›¾ï¼‰â€è¿™ä¸€æ–°æ¨¡æ€ï¼Œåªéœ€å¾®è°ƒ Vision Projector + LLM LoRA

â¸»

1. æ€»ä½“æµç¨‹

GF-5B é«˜å…‰è°±æ•°æ®é›†
     â†“ é¢„å¤„ç†
å…‰è°±æ³¢æ®µç²¾é€‰ / ä¼ªè‰²åˆæˆ / ç‰¹å¾ç¼–ç 
     â†“
æ„é€  Qwen3-VL å¯è¯†åˆ«çš„è§†è§‰è¾“å…¥ï¼ˆimage-like tensorï¼‰
     â†“
æ„å»ºå¤šæ¨¡æ€ç›‘ç£æ•°æ®ï¼ˆimage + prompt + answerï¼‰
     â†“
LoRA å¾®è°ƒï¼ˆVision Projector + éƒ¨åˆ† LLMï¼‰
     â†“
è¯„ä¼°å¹¶å¯¼å‡ºæ¨ç†æ¨¡å‹


â¸»

2. é«˜å…‰è°±æ•°æ® â†’ Qwen3-VL å¯ç”¨å›¾åƒè½¬æ¢

æ–¹å¼ Aï¼šä¼ªå½©è‰² / æ³¢æ®µç»„åˆï¼ˆæœ€æ˜“è½åœ°ï¼‰

GF-5B å…·æœ‰ >300 æ³¢æ®µã€‚
é€‰å–ä¸é“œçŸ¿èš€å˜ç›¸å…³çš„å…³é”®æ³¢æ®µï¼ˆä¾‹å¦‚ 508 nm, 600 nm, 2230 nmâ€¦ï¼‰ï¼Œ
æ˜ å°„ä¸º 3-channel / 6-channel å›¾åƒï¼š
	â€¢	3 é€šé“ï¼šå¸¸è§ä¼ªå½©è‰²ï¼ˆRGBï¼‰
	â€¢	6/8/16 é€šé“ï¼šæ‰©å±•è§†è§‰ç¼–ç å™¨é€šé“æ•°ï¼ˆQwen3 æ”¯æŒå¯è®­ç»ƒ projectorï¼‰

é€‚é…æ–¹æ³•ï¼ˆå®˜æ–¹å…è®¸ï¼‰ï¼š
ä½¿ç”¨ä¸€ä¸ª å¯è®­ç»ƒ 1Ã—1 Conv Projectorï¼ˆç™½çš®ä¹¦æœ‰è¯´æ˜ï¼‰ å°† N é€šé“ â†’ 3 é€šé“
ï¿¼

è¿™æ · Qwen3-VL ä¸éœ€è¦ä¿®æ”¹ä¸»å¹² Vision Transformerã€‚

â¸»

æ–¹å¼ Bï¼šå…‰è°± â†’ embedding â†’ ä¼ªå›¾åƒ

æ„å»ºä¸€ä¸ªå°å‹ CNN/MLPï¼š

spectral(300 bands pixel) â†’ 128-dim embedding
embedding æ’åˆ—æˆ pseudo-image (HÃ—WÃ—128)

ç„¶å projector å°† 128 é€šé“ â†’ Qwen3-VL è§†è§‰ token ç©ºé—´
ğŸ‘‰ æ›´å¼ºï¼Œä½†å·¥ç¨‹æ›´å¤æ‚ã€‚

â¸»

3. å¾®è°ƒä»»åŠ¡ç±»å‹ï¼ˆæ¨èï¼‰

ä»»åŠ¡ 1ï¼šçŸ¿åŒ–åˆ¤åˆ«ï¼ˆåˆ†ç±»ï¼‰

Prompt:

è¯·åˆ¤æ–­è¯¥åŒºåŸŸæ˜¯å¦å­˜åœ¨é“œçŸ¿ç›¸å…³èš€å˜ï¼Ÿ

Answer:

æ˜¯ / å¦

ä»»åŠ¡ 2ï¼šèš€å˜çŸ¿ç‰©è¯†åˆ«ï¼ˆOpen-VQA æ ¼å¼ï¼‰

Prompt:

è¯¥åŒºåŸŸä¸»è¦èš€å˜çŸ¿ç‰©æ˜¯ä»€ä¹ˆï¼Ÿ

Answer:

ç»¿æ³¥çŸ³ + èµ¤é“çŸ¿

ä»»åŠ¡ 3ï¼šçŸ¿åŒ–å¼ºåº¦/æ¦‚ç‡å›å½’ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰

Prompt:

è¾“å‡ºè¯¥åƒå…ƒçš„é“œçŸ¿åŒ–æ¦‚ç‡ï¼ˆ0-1ï¼‰

Answer:

0.87

è¿™äº›ä»»åŠ¡éƒ½é€‚åˆ Qwen3-VL çš„å¤šæ¨¡æ€æŒ‡ä»¤å¾®è°ƒã€‚

â¸»

4. æ•°æ®æ ¼å¼ï¼ˆä¸ Qwen3-VL Instruct å¯¹é½ï¼‰

JSONL æ ¼å¼

{
  "image": "xxx.png",
  "conversations": [
    {"from": "user", "value": "è¯·åˆ¤æ–­è¿™å¹…é«˜å…‰è°±å›¾åƒæ˜¯å¦å­˜åœ¨é“œçŸ¿èš€å˜ï¼Ÿ"},
    {"from": "assistant", "value": "å­˜åœ¨æ˜æ˜¾çš„ç»¿æ³¥çŸ³å’Œèµ¤é“çŸ¿èš€å˜ï¼Œåˆ¤æ–­ä¸ºé“œçŸ¿èš€å˜åŒºã€‚"}
  ]
}

è¿™ä¸ Qwen3-VL å®˜æ–¹ instruct æ ¼å¼ä¿æŒä¸€è‡´ã€‚

â¸»

5. å¾®è°ƒç­–ç•¥ï¼ˆé‡ç‚¹ï¼‰

æ¨èç­–ç•¥ï¼šVision Projector + LLM LoRA è”åˆå¾®è°ƒ

åŸå› ï¼š

âœ” Vision Projector è´Ÿè´£â€œæ–°æ¨¡æ€å¯¹é½â€
âœ” LoRA è´Ÿè´£è¯­è¨€ä»»åŠ¡å­¦ä¹ 
âœ” æ˜¾å­˜ä½ã€è®­ç»ƒå¿«ã€æ•ˆæœæ›´ç¨³å®š

å†»ç»“éƒ¨åˆ†æ¨¡å—ï¼š
	â€¢	Vision Encoderï¼ˆViT ä¸»å¹²ï¼‰â„ å†»ç»“
	â€¢	LLM ä¸»ä½“ï¼ˆTransformer blockï¼‰â„ å†»ç»“
	â€¢	å¯è®­ç»ƒéƒ¨åˆ†ï¼š
	â€¢	Multi-modal Projector ï¼ˆç™½çš®ä¹¦æ˜ç¡®å…è®¸ï¼‰
	â€¢	LLM çš„éƒ¨åˆ†æ³¨æ„åŠ›å±‚ï¼ˆLoRAï¼‰

â¸»

6. å¾®è°ƒå‚æ•°ï¼ˆå¯ç›´æ¥ä½¿ç”¨ï¼‰

é…ç½®	æ¨èå€¼
æ¨¡å‹	Qwen3-VL-8B-Instruct
å¾®è°ƒæ–¹å¼	LoRA + Projector FT
batch size	4â€“16
lr	projector: 1e-4ï¼›LoRA: 2e-5
å›¾åƒå°ºå¯¸	448Ã—448 / 672Ã—672
æ–‡æœ¬ max_tokens	2048
GPU æ¨è	A100 40G Ã—1 / H20 Ã—1 / 4090 Ã—2


â¸»

7. è®­ç»ƒè„šæœ¬ï¼ˆå¯ç›´æ¥è¿è¡Œï¼‰

ä¸‹é¢ç»™å‡º å¯ç›´æ¥è¿è¡Œçš„è®­ç»ƒæ¨¡æ¿ï¼ˆPyTorch + Transformersï¼‰ï¼š

train.py

from transformers import AutoProcessor, AutoModelForVision2Seq, TrainingArguments, Trainer
from peft import LoraConfig, get_peft_model
import datasets

model_name = "Qwen/Qwen3-VL-8B-Instruct"

processor = AutoProcessor.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForVision2Seq.from_pretrained(model_name, trust_remote_code=True)

# LoRA for LLM
lora = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_dropout=0.1
)
model = get_peft_model(model, lora)

# å…è®¸ projector è®­ç»ƒ
for name, param in model.named_parameters():
    if "vision_proj" in name:
        param.requires_grad = True

# Dataset
dataset = datasets.load_dataset("json", data_files="train.jsonl")["train"]

def collate_fn(batch):
    return processor(batch, return_tensors="pt")

training_args = TrainingArguments(
    output_dir="./qwen3-vl-finetune",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    fp16=True,
    num_train_epochs=5,
    logging_steps=20,
    save_steps=1000,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset,
    data_collator=collate_fn,
)

trainer.train()


â¸»

8. æ¨ç†ï¼ˆInfernceï¼‰ç¤ºä¾‹

inputs = processor(
    images=image,
    text="è¯¥åŒºåŸŸæ˜¯å¦å­˜åœ¨é“œçŸ¿èš€å˜ï¼Ÿ",
    return_tensors="pt"
)

output = model.generate(**inputs, max_new_tokens=256)
print(processor.decode(output[0]))


â¸»

9. å¯å®ç°çš„åˆ›æ–°ç‚¹ï¼ˆé¡¹ç›®ä¹¦å¯ç”¨ï¼‰
	1.	é«˜å…‰è°± â†’ å¤šæ¨¡æ€å¤§æ¨¡å‹çš„æ¨¡æ€è‡ªé€‚åº”æ˜ å°„
	2.	Projector ä¸ LoRA ååŒå¾®è°ƒç”¨äºåœ°è´¨é¥æ„Ÿ
	3.	çŸ¿åŒ–ç»“æ„åŒ–æè¿°ç”Ÿæˆï¼ˆSLM â†’ Struct outputï¼‰
	4.	çŸ¿åŒ–æ¦‚ç‡çƒ­å›¾åˆ¶å¤‡ï¼ˆprompt + token-level è¾“å‡ºï¼‰
	5.	æ— æ ‡æ³¨åŒºå¯é€šè¿‡ GPT-based ç”Ÿæˆå¼å¼±ç›‘ç£å¢å¼º


ä¸€ã€å¯ç›´æ¥ç”¨äºâ€œæ‰¾çŸ¿ / èš€å˜è¯†åˆ«â€ä»»åŠ¡çš„å¼€æºé«˜å…‰è°±æ•°æ®é›†ï¼ˆæœ€é‡è¦ï¼‰

1. Cupriteï¼ˆç¾å›½ Cuprite é“œçŸ¿åŒºï¼‰â€” ä¸–ç•Œæœ€ç»å…¸æ‰¾çŸ¿é«˜å…‰è°±æ•°æ®é›†ï¼ˆå¼ºçƒˆæ¨èï¼‰

ğŸ“Œ å®Œå…¨å¼€æºã€æœ‰çŸ¿ç‰©æ ‡æ³¨ã€æœ‰æ‰¾çŸ¿ä»·å€¼ã€æœ‰åœ°è´¨çœŸå€¼
ğŸ“Œ å¯å®Œç¾ç”¨äºä½ è¦åšçš„â€œé“œçŸ¿èš€å˜è¯†åˆ« + çŸ¿åŒ–æ¨ç†â€

æ¥æº
	â€¢	AVIRIS / HyMap æ•°æ®
	â€¢	ç¾å›½åœ°è°ƒå±€ USGS å…¬å¼€
	â€¢	åœ°è´¨ä¸“å®¶ç»™å‡ºçš„èš€å˜å¸¦æ ‡æ³¨ï¼ˆç™½äº‘æ¯ã€ç»¿æ³¥çŸ³ã€èµ¤é“çŸ¿ç­‰ï¼‰

ä»»åŠ¡å¯ä»¥åšï¼š
	â€¢	èš€å˜çŸ¿ç‰©åˆ†ç±»ï¼ˆferric iron, chlorite, aluniteâ€¦ï¼‰
	â€¢	é“œçŸ¿èš€å˜å¸¦æ¨æ–­
	â€¢	å…‰è°±ç‰¹å¾â†’å¤šæ¨¡æ€ä»»åŠ¡ï¼ˆä½ çš„æ¨¡å‹ç”¨ï¼‰

å®Œå…¨åŒ¹é…ä½ çš„é¡¹ç›®ï¼Œæ˜¯â€œæ‰¾é“œçŸ¿æ¨¡å‹éªŒè¯â€çš„æœ€ä½³å¼€æºæ•°æ®é›†ã€‚